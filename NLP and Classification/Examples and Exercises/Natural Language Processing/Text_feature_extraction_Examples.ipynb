{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpIPMNPoU_-p"
   },
   "source": [
    "# Examples: Text feature extraction\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will delve into text feature extraction techniques, focusing on the bag-of-words model and n-grams. We'll explore how to transform text data into feature sets usable by classifiers, particularly using the NLTK library. The bag-of-words model simplifies text into word presence features, while n-grams capture combinations of words to extract deeper meaning from text. \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "* Understand the bag-of-words model and its role in text feature extraction.\n",
    "* Implement the bag-of-words model to transform text data into feature sets.\n",
    "* Explain the concept of n-grams and their significance in capturing combinations of words.\n",
    "* Use n-grams to extract contextual information from text data.\n",
    "* Fine-tune CountVectorizer parameters for optimal text feature extraction.\n",
    "\n",
    "\n",
    "Before we get started, let's get the data and the  libraries we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "# Set the path to the CA certificates bundle\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQm0O5XHU_-z"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "import string\n",
    "\n",
    "# set plot style\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29356,
     "status": "error",
     "timestamp": 1560340175121,
     "user": {
      "displayName": "Bryan Davies",
      "photoUrl": "",
      "userId": "03059035420523728518"
     },
     "user_tz": -120
    },
    "id": "w8Iw1yCRU_-2",
    "outputId": "188501d1-fcf6-45be-8a45-56f885491dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()\n",
    "# or you can download directly, i.e.\n",
    "#nltk.download(['punkt','stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with our `MBTI` dataset, let's read the data and clean it up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the MBTI dataset\n",
    "mbti = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint/mbti_train.csv')\n",
    "mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://41.media.tumblr.com/tumblr_lfouy03PMA1q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  https://www.youtube.com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://www.youtube.com/watch?v=vXZeYwwRDw8   h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316543</th>\n",
       "      <td>INFP</td>\n",
       "      <td>Kallinhausin, you may have just rooted out the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316544</th>\n",
       "      <td>INFP</td>\n",
       "      <td>In regards to the king, (in the show, not in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316545</th>\n",
       "      <td>INFP</td>\n",
       "      <td>Sunlight bouncing off the fog at dawn.  Serend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316546</th>\n",
       "      <td>INFP</td>\n",
       "      <td>Songs are really powerful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316547</th>\n",
       "      <td>INFP</td>\n",
       "      <td>I just have to remember they weren't trying to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316548 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                                               post\n",
       "0       INFJ        'http://www.youtube.com/watch?v=qsXHcwe3krw\n",
       "1       INFJ  http://41.media.tumblr.com/tumblr_lfouy03PMA1q...\n",
       "2       INFJ  enfp and intj moments  https://www.youtube.com...\n",
       "3       INFJ  What has been the most life-changing experienc...\n",
       "4       INFJ  http://www.youtube.com/watch?v=vXZeYwwRDw8   h...\n",
       "...      ...                                                ...\n",
       "316543  INFP  Kallinhausin, you may have just rooted out the...\n",
       "316544  INFP  In regards to the king, (in the show, not in t...\n",
       "316545  INFP  Sunlight bouncing off the fog at dawn.  Serend...\n",
       "316546  INFP                         Songs are really powerful.\n",
       "316547  INFP  I just have to remember they weren't trying to...\n",
       "\n",
       "[316548 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate each post in the 'posts' column into its own row\n",
    "all_mbti = []\n",
    "for i, row in mbti.iterrows():\n",
    "    for post in row['posts'].split('|||'):\n",
    "        all_mbti.append([row['type'], post])\n",
    "all_mbti = pd.DataFrame(all_mbti, columns=['type', 'post'])\n",
    "\n",
    "all_mbti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove noise\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "all_mbti['post'] = all_mbti['post'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "all_mbti['post'] = all_mbti['post'].str.lower()\n",
    "\n",
    "#Remove puntuation\n",
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])\n",
    "\n",
    "all_mbti['post'] = all_mbti['post'].apply(remove_punctuation)\n",
    "\n",
    "# Tokenize the text using the TreebankWordTokenizer\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "all_mbti['tokens'] = all_mbti['post'].apply(tokeniser.tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text feature extraction\n",
    "\n",
    "### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BbRb9FHYVAAg"
   },
   "source": [
    "Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier. The NLTK classifiers expect `dict` style feature sets, so we must therefore transform our text into a Python dictionary object. The Bag of Words model is the simplest method; it constructs a word presence feature set from all the words in the text, indicating the number of times each word has appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mg8PtprJVAAg"
   },
   "outputs": [],
   "source": [
    "def bag_of_words_count(words, word_dict={}):\n",
    "    \"\"\" this function takes in a list of words and returns a dictionary \n",
    "        with each word as a key, and the value represents the number of \n",
    "        times that word appeared\"\"\"\n",
    "    for word in words:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a set of dictionaries, one for each of the MBTI types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INFJ',\n",
       " 'ENTP',\n",
       " 'INTP',\n",
       " 'INTJ',\n",
       " 'ENTJ',\n",
       " 'ENFJ',\n",
       " 'INFP',\n",
       " 'ENFP',\n",
       " 'ISFP',\n",
       " 'ISTP',\n",
       " 'ISFJ',\n",
       " 'ISTJ',\n",
       " 'ESTP',\n",
       " 'ESFP',\n",
       " 'ESTJ',\n",
       " 'ESFJ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a list of all the MBTI personality types that are present in the original dataset\n",
    "type_labels = list(all_mbti.type.unique())\n",
    "type_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtqJO_YhVAAh"
   },
   "outputs": [],
   "source": [
    "personality = {}\n",
    "for pp in type_labels:\n",
    "    df = all_mbti.groupby('type')\n",
    "    personality[pp] = {}\n",
    "    for row in df.get_group(pp)['tokens']:\n",
    "        personality[pp] = bag_of_words_count(row, personality[pp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a list of all of the unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ky-rofa_VAAi"
   },
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "for pp in type_labels:\n",
    "    for word in personality[pp]:\n",
    "        all_words.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was done so that we can create a combined bag of words dictionary for all the words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpjTsIOsVAAl"
   },
   "outputs": [],
   "source": [
    "personality['all'] = {}\n",
    "for pp in type_labels:    \n",
    "    for word in all_words:\n",
    "        if word in personality[pp].keys():\n",
    "            if word in personality['all']:\n",
    "                personality['all'][word] += personality[pp][word]\n",
    "            else:\n",
    "                personality['all'][word] = personality[pp][word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily calculate how many words there are in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUGBaAELVAAp",
    "outputId": "beff7596-19e4-43bb-ecce-291231be4c21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8203466"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = sum([v for v in personality['all'].values()])\n",
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of words which occur less than 10 times in the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aiSrw4V3VAAn",
    "outputId": "b59469fe-8252-4546-94b3-76c1d9ac59bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'word frequency')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist([v for v in personality['all'].values() if v < 10],bins=10)\n",
    "plt.ylabel(\"# of words\")\n",
    "plt.xlabel(\"word frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QspSnpFzVAAp"
   },
   "source": [
    "There are a lot of words that only appear once! We'll print out that value here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBsFSL29VAAw",
    "outputId": "64e625a1-15be-4541-b802-632e803b903a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81268"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([v for v in personality['all'].values() if v == 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of words do you think would appear once? Let's print out a few of these rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['confusedquestioning', 'rhgs', 'passsion', 'notquitethere', 'miraclemiraculous', 'pangolins', 'freedomphase', 'infinitewhen', 'germanium', 'tientp', '177602', 'supastition', 'beskriver', 'sinker', 'nonfat', 'fecalmater', 'tenants', 'cronk', 'stomaching', 'ani', 'purposive', 'walketh', 'siomai', 'getwell', 'kyn', 'kyousuke', 'gocause', 'butchers', 'twothis', 'presumptuouspretentious', 'unaccurate', 'gjort', 'technoviking', 'kindersurprises', 'biliana', 'emoji28emoji1362', 'tepe', '16hour', 'coysince', '16000', 'handrail', 'approprate', 'duplicitious', 'mythologicalmystic', 'méxico', 'mitzi', 'smashingly', 'laboured', 'occassionlly', 'indont', 'sad13th', '9780140126785', 'gesticular', 'postspm', 'sharpend', 'solomons', 'grapeseed', 'hahahxd', 'handmedown', 'aerate', 'melodyvideo', 'hemingwayesque', 'lyricslines', 'expextation', 'auurrghhhh', 'tantalizingly', 'extjoriented', 'meconfused', 'latterly', 'potbelly', 'subsuming', 'gnihihi', 'blushedtongue', 'sworm09', 'competely', 'delet', 'whyplayrisk', 'handcrafted', 'wren', 'eachs', 'ooma', 'tolding', 'cassadee', 'soon—but', 'threadop', 'zorrobean', 'yknowcompletely', 'simultaniously', 'belbin', 'romane', 'qing', 'mural', 'againsomeone', 'musicsong', 'bragtime', 'betterpaying', 'guava', 'theirenfjs', 'antienjoyable', '588066']\n"
     ]
    }
   ],
   "source": [
    "rare_words = [k for k, v in personality['all'].items() if v==1] \n",
    "print(rare_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some of these words don't make sense, but before we decide to remove them, let's see how much data we'll be left with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amu9ABO8VAAz",
    "outputId": "3d99bd9f-f5eb-4c26-f3e4-d1751eee8920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18193\n",
      "7998579\n"
     ]
    }
   ],
   "source": [
    "# how many words appear more than 10 times?\n",
    "# how many words of the total does that account for?\n",
    "print(len([v for v in personality['all'].values() if v >= 10]))\n",
    "occurs_more_than_10_times = sum([v for v in personality['all'].values() if v >= 10])\n",
    "print(occurs_more_than_10_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwmyzJewVAA2",
    "outputId": "7e64933d-1034-4c8f-d160-36472eeb154f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9750243372740254"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurs_more_than_10_times/total_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxW-TeheVAA4"
   },
   "source": [
    "Using words that appear more than 10 times seems much more useful!  And this accounts for 97% of all the words!\n",
    "\n",
    "Finally, let's remove all words that occur less than 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZNdn2n6VAA4"
   },
   "outputs": [],
   "source": [
    "max_count = 10\n",
    "remaining_word_index = [k for k, v in personality['all'].items() if v > max_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing\n",
    "Remember our Hypothesis from earlier?:\n",
    "\n",
    "- Introverts tend to use the word `I` more than extroverts\n",
    "- Conversely, Extroverts tend to favour the word `you`\n",
    "\n",
    "Let's see if we finally have what we need to test it out. We'll first create one big dataframe with the word counts by personality profile (this may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fW5qiwvrVAA5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 INFJ\n",
      "Word                 \n",
      "urlweb           3576\n",
      "enfp              702\n",
      "and             31628\n",
      "intj              984\n",
      "moments           123\n",
      "...               ...\n",
      "offends             1\n",
      "stefan              1\n",
      "unprofessional      1\n",
      "proceeds            1\n",
      "voicemail           1\n",
      "\n",
      "[16415 rows x 1 columns]\n",
      "              ENTP\n",
      "Word              \n",
      "im            4518\n",
      "finding         68\n",
      "the          18993\n",
      "lack           140\n",
      "of           11335\n",
      "...            ...\n",
      "pennies          1\n",
      "infjennifer      3\n",
      "bundle           1\n",
      "ab               1\n",
      "18th             1\n",
      "\n",
      "[14802 rows x 1 columns]\n",
      "                INTP\n",
      "Word                \n",
      "good            2076\n",
      "one             3932\n",
      "urlweb          3277\n",
      "of             21372\n",
      "course           392\n",
      "...              ...\n",
      "extjs              1\n",
      "culinary           1\n",
      "broadway           2\n",
      "charlieelliot      1\n",
      "intrapersonal      1\n",
      "\n",
      "[16222 rows x 1 columns]\n",
      "          INTJ\n",
      "Word          \n",
      "dear       152\n",
      "intp       658\n",
      "i        43810\n",
      "enjoyed     76\n",
      "our        774\n",
      "...        ...\n",
      "neil         1\n",
      "olivia       1\n",
      "spektor      1\n",
      "evan         1\n",
      "grieve       1\n",
      "\n",
      "[15873 rows x 1 columns]\n",
      "            ENTJ\n",
      "Word            \n",
      "youre        340\n",
      "fired          7\n",
      "thats        319\n",
      "another      117\n",
      "silly         23\n",
      "...          ...\n",
      "positives      1\n",
      "downright      1\n",
      "constructs     1\n",
      "simon          1\n",
      "oi             1\n",
      "\n",
      "[10184 rows x 1 columns]\n",
      "               ENFJ\n",
      "Word               \n",
      "urlweb          384\n",
      "51                1\n",
      "o                10\n",
      "i              8683\n",
      "went             55\n",
      "...             ...\n",
      "prerequisites     1\n",
      "discipline        1\n",
      "ambivalent        1\n",
      "misogyny          2\n",
      "doesn’t           1\n",
      "\n",
      "[9214 rows x 1 columns]\n",
      "               INFP\n",
      "Word               \n",
      "i             87642\n",
      "think          8471\n",
      "we             4862\n",
      "do             7730\n",
      "agree          1066\n",
      "...             ...\n",
      "sealed            1\n",
      "shouted           1\n",
      "moaning           1\n",
      "entrepreneur      1\n",
      "ditched           1\n",
      "\n",
      "[16720 rows x 1 columns]\n",
      "          ENFP\n",
      "Word          \n",
      "he        2034\n",
      "doesnt     461\n",
      "want      1154\n",
      "to       16945\n",
      "go         917\n",
      "...        ...\n",
      "gears        2\n",
      "mutant       1\n",
      "loooove      1\n",
      "academy      1\n",
      "smilies      1\n",
      "\n",
      "[14132 rows x 1 columns]\n",
      "            ISFP\n",
      "Word            \n",
      "they         680\n",
      "paint         21\n",
      "without       92\n",
      "numbers        7\n",
      "id           244\n",
      "...          ...\n",
      "bane           1\n",
      "cooks          1\n",
      "finale         1\n",
      "miserables     1\n",
      "diablo         1\n",
      "\n",
      "[10368 rows x 1 columns]\n",
      "                 ISTP\n",
      "Word                 \n",
      "i               13883\n",
      "got               425\n",
      "from              849\n",
      "what             1477\n",
      "ive               838\n",
      "...               ...\n",
      "lifted              1\n",
      "grid                1\n",
      "hahahahahahaha      1\n",
      "bland               1\n",
      "brb                 1\n",
      "\n",
      "[11801 rows x 1 columns]\n",
      "              ISFJ\n",
      "Word              \n",
      "i             8037\n",
      "love           339\n",
      "feeling         82\n",
      "affectionate     4\n",
      "for           1435\n",
      "...            ...\n",
      "yoda             1\n",
      "pls              3\n",
      "analytics        1\n",
      "rad              1\n",
      "bloody           1\n",
      "\n",
      "[8587 rows x 1 columns]\n",
      "               ISTJ\n",
      "Word               \n",
      "universal         2\n",
      "gravity           3\n",
      "law              18\n",
      "i              8169\n",
      "mean            128\n",
      "...             ...\n",
      "atomic            1\n",
      "fills             1\n",
      "unconditional     1\n",
      "varieties         1\n",
      "gallery           1\n",
      "\n",
      "[9469 rows x 1 columns]\n",
      "              ESTP\n",
      "Word              \n",
      "cell             1\n",
      "for            763\n",
      "xbox             1\n",
      "360              1\n",
      "estps          109\n",
      "...            ...\n",
      "offbeat          1\n",
      "former           1\n",
      "abstractions     1\n",
      "ranting          1\n",
      "runner           1\n",
      "\n",
      "[6710 rows x 1 columns]\n",
      "        ESFP\n",
      "Word        \n",
      "edit       7\n",
      "i       1696\n",
      "forgot     4\n",
      "what     220\n",
      "board      1\n",
      "...      ...\n",
      "sky        1\n",
      "dreary     1\n",
      "hole       1\n",
      "grieve     1\n",
      "poet       1\n",
      "\n",
      "[4329 rows x 1 columns]\n",
      "           ESTJ\n",
      "Word           \n",
      "this        280\n",
      "is          554\n",
      "such         21\n",
      "a           841\n",
      "catch         1\n",
      "...         ...\n",
      "published     1\n",
      "aim           1\n",
      "previous      1\n",
      "eccentric     1\n",
      "greetings     1\n",
      "\n",
      "[4438 rows x 1 columns]\n",
      "           ESFJ\n",
      "Word           \n",
      "why          68\n",
      "not         281\n",
      "any          72\n",
      "other       101\n",
      "esfjs       118\n",
      "...         ...\n",
      "benefits      1\n",
      "pressures     1\n",
      "embraced      1\n",
      "gf            1\n",
      "setup         1\n",
      "\n",
      "[4546 rows x 1 columns]\n",
      "            all\n",
      "Word           \n",
      "inc          16\n",
      "beethoven    33\n",
      "prominent    48\n",
      "entx         29\n",
      "filter       88\n",
      "...         ...\n",
      "hughugs      11\n",
      "smg360v      26\n",
      "bobtoeback   16\n",
      "gts5300      11\n",
      "jetplane48   12\n",
      "\n",
      "[17165 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "hm = []\n",
    "for p, p_bow in personality.items():\n",
    "    df_bow = pd.DataFrame([(k, v) for k, v in p_bow.items() if k in remaining_word_index], columns=['Word', p])\n",
    "    df_bow.set_index('Word', inplace=True)\n",
    "    hm.append(df_bow)\n",
    "    print(df_bow)\n",
    "\n",
    "# create one big dataframe\n",
    "df_bow = pd.concat(hm, axis=1)\n",
    "df_bow.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 words which appear most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaKScx05VAA7",
    "outputId": "5bcaa326-858b-478f-9f88-f582ce599a20"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>INTP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>67821.0</td>\n",
       "      <td>27381.0</td>\n",
       "      <td>52046.0</td>\n",
       "      <td>43810.0</td>\n",
       "      <td>8875.0</td>\n",
       "      <td>8683.0</td>\n",
       "      <td>87642.0</td>\n",
       "      <td>31156.0</td>\n",
       "      <td>11148.0</td>\n",
       "      <td>13883.0</td>\n",
       "      <td>8037.0</td>\n",
       "      <td>8169.0</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>1696.0</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>2166.0</td>\n",
       "      <td>378073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>39658.0</td>\n",
       "      <td>18993.0</td>\n",
       "      <td>35864.0</td>\n",
       "      <td>30497.0</td>\n",
       "      <td>6132.0</td>\n",
       "      <td>5018.0</td>\n",
       "      <td>48004.0</td>\n",
       "      <td>16454.0</td>\n",
       "      <td>6131.0</td>\n",
       "      <td>8893.0</td>\n",
       "      <td>4111.0</td>\n",
       "      <td>5141.0</td>\n",
       "      <td>2191.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>230224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>40231.0</td>\n",
       "      <td>17852.0</td>\n",
       "      <td>33005.0</td>\n",
       "      <td>28753.0</td>\n",
       "      <td>5889.0</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>48996.0</td>\n",
       "      <td>16945.0</td>\n",
       "      <td>6264.0</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>4607.0</td>\n",
       "      <td>5106.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>227371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>31931.0</td>\n",
       "      <td>14728.0</td>\n",
       "      <td>26692.0</td>\n",
       "      <td>22778.0</td>\n",
       "      <td>4748.0</td>\n",
       "      <td>3966.0</td>\n",
       "      <td>40375.0</td>\n",
       "      <td>13846.0</td>\n",
       "      <td>4825.0</td>\n",
       "      <td>7124.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>4033.0</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>841.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>182870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>31628.0</td>\n",
       "      <td>14236.0</td>\n",
       "      <td>24880.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>4564.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>40709.0</td>\n",
       "      <td>15002.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>6540.0</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>3827.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>988.0</td>\n",
       "      <td>180691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>24312.0</td>\n",
       "      <td>11335.0</td>\n",
       "      <td>21372.0</td>\n",
       "      <td>17857.0</td>\n",
       "      <td>3499.0</td>\n",
       "      <td>3114.0</td>\n",
       "      <td>29576.0</td>\n",
       "      <td>10217.0</td>\n",
       "      <td>3580.0</td>\n",
       "      <td>4962.0</td>\n",
       "      <td>2475.0</td>\n",
       "      <td>2976.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>138561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>22207.0</td>\n",
       "      <td>10870.0</td>\n",
       "      <td>17186.0</td>\n",
       "      <td>15997.0</td>\n",
       "      <td>3815.0</td>\n",
       "      <td>3048.0</td>\n",
       "      <td>24954.0</td>\n",
       "      <td>10315.0</td>\n",
       "      <td>3331.0</td>\n",
       "      <td>4696.0</td>\n",
       "      <td>2185.0</td>\n",
       "      <td>2731.0</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>124672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>19444.0</td>\n",
       "      <td>8947.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>14432.0</td>\n",
       "      <td>2906.0</td>\n",
       "      <td>2611.0</td>\n",
       "      <td>23437.0</td>\n",
       "      <td>8583.0</td>\n",
       "      <td>2931.0</td>\n",
       "      <td>4054.0</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>2207.0</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>110718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>18357.0</td>\n",
       "      <td>8257.0</td>\n",
       "      <td>15685.0</td>\n",
       "      <td>13167.0</td>\n",
       "      <td>2601.0</td>\n",
       "      <td>2278.0</td>\n",
       "      <td>22511.0</td>\n",
       "      <td>8010.0</td>\n",
       "      <td>2909.0</td>\n",
       "      <td>4244.0</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>104808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>18237.0</td>\n",
       "      <td>8903.0</td>\n",
       "      <td>15888.0</td>\n",
       "      <td>14293.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2404.0</td>\n",
       "      <td>21068.0</td>\n",
       "      <td>7769.0</td>\n",
       "      <td>2726.0</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>1879.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>104779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         INFJ     ENTP     INTP     INTJ    ENTJ    ENFJ     INFP     ENFP  \\\n",
       "Word                                                                         \n",
       "i     67821.0  27381.0  52046.0  43810.0  8875.0  8683.0  87642.0  31156.0   \n",
       "the   39658.0  18993.0  35864.0  30497.0  6132.0  5018.0  48004.0  16454.0   \n",
       "to    40231.0  17852.0  33005.0  28753.0  5889.0  5471.0  48996.0  16945.0   \n",
       "a     31931.0  14728.0  26692.0  22778.0  4748.0  3966.0  40375.0  13846.0   \n",
       "and   31628.0  14236.0  24880.0  21568.0  4564.0  4343.0  40709.0  15002.0   \n",
       "of    24312.0  11335.0  21372.0  17857.0  3499.0  3114.0  29576.0  10217.0   \n",
       "you   22207.0  10870.0  17186.0  15997.0  3815.0  3048.0  24954.0  10315.0   \n",
       "that  19444.0   8947.0  16384.0  14432.0  2906.0  2611.0  23437.0   8583.0   \n",
       "it    18357.0   8257.0  15685.0  13167.0  2601.0  2278.0  22511.0   8010.0   \n",
       "is    18237.0   8903.0  15888.0  14293.0  3000.0  2404.0  21068.0   7769.0   \n",
       "\n",
       "         ISFP     ISTP    ISFJ    ISTJ    ESTP    ESFP    ESTJ    ESFJ     all  \n",
       "Word                                                                            \n",
       "i     11148.0  13883.0  8037.0  8169.0  3704.0  1696.0  1856.0  2166.0  378073  \n",
       "the    6131.0   8893.0  4111.0  5141.0  2191.0   937.0  1000.0  1200.0  230224  \n",
       "to     6264.0   8725.0  4607.0  5106.0  2254.0   972.0  1078.0  1223.0  227371  \n",
       "a      4825.0   7124.0  3333.0  4033.0  1868.0   796.0   841.0   986.0  182870  \n",
       "and    5153.0   6540.0  3571.0  3827.0  1905.0   834.0   943.0   988.0  180691  \n",
       "of     3580.0   4962.0  2475.0  2976.0  1300.0   557.0   650.0   779.0  138561  \n",
       "you    3331.0   4696.0  2185.0  2731.0  1396.0   651.0   651.0   639.0  124672  \n",
       "that   2931.0   4054.0  2033.0  2207.0  1063.0   550.0   521.0   615.0  110718  \n",
       "it     2909.0   4244.0  2046.0  2285.0  1065.0   434.0   469.0   490.0  104808  \n",
       "is     2726.0   3704.0  1879.0  2186.0  1121.0   482.0   554.0   565.0  104779  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow.sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MTEdKrZVAA8"
   },
   "source": [
    "This isn't very helpful at all, is it? It's very difficult to extract insights from this data.  Let's see if we can use the $chi^2$ test to see whether Introverts favour the word **`I`**. \n",
    "\n",
    "The $chi^2$ test looks at observed versus expected results and lets us know where the greatest differences from expected values are.  The bigger the statistic, the greater the difference from expectation.  The formula is \n",
    "\n",
    "$$𝑐ℎ𝑖^2 = \\sum{\\frac{(𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑 −𝑒𝑥𝑝𝑒𝑐𝑡𝑒𝑑)^2}{𝑒𝑥𝑝𝑒𝑐𝑡𝑒𝑑}}$$\n",
    "\n",
    "The $chi^2$ test will compare the **observed frequencies** of word usage by **introverts** to the **expected frequencies** based on the overall population and indicate the extent of this difference for each word.\n",
    "\n",
    "Using the $chi^2$ statistic over simply comparing the observed percentages, i.e `I_perc`, means that we are considering both the observed (or word usage by introverts) and expected frequencies(or the overall populations word usage) for each word, taking into account the sample size. This helps us determine whether the differences between observed and expected frequencies are statistically significant, accounting for variability due to sample size.\n",
    "\n",
    "We'll do this first by extracting introvert types only from all the personality types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2eTstsUzVAA8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INFJ', 'INTP', 'INTJ', 'INFP', 'ISFP', 'ISTP', 'ISFJ', 'ISTJ']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_types_i = [p for p in type_labels if p[0] == 'I']\n",
    "intro_types_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create an introvert total word count column, which sums the counts of all introvert columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7F3a0V80VAA_"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>INTP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>all</th>\n",
       "      <th>I</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>urlweb</th>\n",
       "      <td>3576.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>3277.0</td>\n",
       "      <td>2619.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>5057.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>911.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>20887</td>\n",
       "      <td>17279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfp</th>\n",
       "      <td>702.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>2134.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5814</td>\n",
       "      <td>2786.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>31628.0</td>\n",
       "      <td>14236.0</td>\n",
       "      <td>24880.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>4564.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>40709.0</td>\n",
       "      <td>15002.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>6540.0</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>3827.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>988.0</td>\n",
       "      <td>180691</td>\n",
       "      <td>137876.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intj</th>\n",
       "      <td>984.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>3160.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7806</td>\n",
       "      <td>6217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moments</th>\n",
       "      <td>123.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>540</td>\n",
       "      <td>421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smg360v</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bobtoeback</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hughugs</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gts5300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jetplane48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17165 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               INFJ     ENTP     INTP     INTJ    ENTJ    ENFJ     INFP  \\\n",
       "Word                                                                      \n",
       "urlweb       3576.0   1250.0   3277.0   2619.0   382.0   384.0   5057.0   \n",
       "enfp          702.0    483.0    337.0    541.0   123.0   117.0    798.0   \n",
       "and         31628.0  14236.0  24880.0  21568.0  4564.0  4343.0  40709.0   \n",
       "intj          984.0    516.0    846.0   3160.0   224.0   110.0    798.0   \n",
       "moments       123.0     41.0     68.0     70.0    10.0    16.0    112.0   \n",
       "...             ...      ...      ...      ...     ...     ...      ...   \n",
       "smg360v         0.0      0.0      0.0      0.0     0.0     0.0      0.0   \n",
       "bobtoeback      0.0      0.0      0.0      0.0     0.0     0.0      0.0   \n",
       "hughugs         0.0      0.0      0.0      0.0     0.0     0.0      0.0   \n",
       "gts5300         0.0      0.0      0.0      0.0     0.0     0.0      0.0   \n",
       "jetplane48      0.0      0.0      0.0      0.0     0.0     0.0      0.0   \n",
       "\n",
       "               ENFP    ISFP    ISTP    ISFJ    ISTJ    ESTP   ESFP   ESTJ  \\\n",
       "Word                                                                        \n",
       "urlweb       1180.0   911.0  1027.0   410.0   402.0   225.0  104.0   36.0   \n",
       "enfp         2134.0   147.0   104.0    53.0   104.0    55.0   39.0   42.0   \n",
       "and         15002.0  5153.0  6540.0  3571.0  3827.0  1905.0  834.0  943.0   \n",
       "intj          576.0   105.0   150.0    77.0    97.0    65.0   41.0   23.0   \n",
       "moments        39.0    11.0    21.0     9.0     7.0     6.0    3.0    0.0   \n",
       "...             ...     ...     ...     ...     ...     ...    ...    ...   \n",
       "smg360v         0.0    26.0     0.0     0.0     0.0     0.0    0.0    0.0   \n",
       "bobtoeback      0.0    10.0     0.0     6.0     0.0     0.0    0.0    0.0   \n",
       "hughugs         0.0    11.0     0.0     0.0     0.0     0.0    0.0    0.0   \n",
       "gts5300         0.0     0.0    11.0     0.0     0.0     0.0    0.0    0.0   \n",
       "jetplane48      0.0     0.0     0.0     0.0     0.0    12.0    0.0    0.0   \n",
       "\n",
       "             ESFJ     all         I  \n",
       "Word                                 \n",
       "urlweb       47.0   20887   17279.0  \n",
       "enfp         35.0    5814    2786.0  \n",
       "and         988.0  180691  137876.0  \n",
       "intj         34.0    7806    6217.0  \n",
       "moments       4.0     540     421.0  \n",
       "...           ...     ...       ...  \n",
       "smg360v       0.0      26      26.0  \n",
       "bobtoeback    0.0      16      16.0  \n",
       "hughugs       0.0      11      11.0  \n",
       "gts5300       0.0      11      11.0  \n",
       "jetplane48    0.0      12       0.0  \n",
       "\n",
       "[17165 rows x 18 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow['I'] = df_bow[intro_types_i].sum(axis=1)\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll calculate and add percentage columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tESewJzZVABA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>INTP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>all</th>\n",
       "      <th>I</th>\n",
       "      <th>I_perc</th>\n",
       "      <th>all_perc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>urlweb</th>\n",
       "      <td>3576.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>3277.0</td>\n",
       "      <td>2619.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>5057.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>911.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>20887</td>\n",
       "      <td>17279.0</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.002615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfp</th>\n",
       "      <td>702.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>2134.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5814</td>\n",
       "      <td>2786.0</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>31628.0</td>\n",
       "      <td>14236.0</td>\n",
       "      <td>24880.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>4564.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>40709.0</td>\n",
       "      <td>15002.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>6540.0</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>3827.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>988.0</td>\n",
       "      <td>180691</td>\n",
       "      <td>137876.0</td>\n",
       "      <td>0.022481</td>\n",
       "      <td>0.022619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intj</th>\n",
       "      <td>984.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>3160.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7806</td>\n",
       "      <td>6217.0</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moments</th>\n",
       "      <td>123.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>540</td>\n",
       "      <td>421.0</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smg360v</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bobtoeback</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hughugs</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gts5300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jetplane48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17165 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               INFJ     ENTP     INTP     INTJ    ENTJ    ENFJ     INFP  \\\n",
       "Word                                                                      \n",
       "urlweb       3576.0   1250.0   3277.0   2619.0   382.0   384.0   5057.0   \n",
       "enfp          702.0    483.0    337.0    541.0   123.0   117.0    798.0   \n",
       "and         31628.0  14236.0  24880.0  21568.0  4564.0  4343.0  40709.0   \n",
       "intj          984.0    516.0    846.0   3160.0   224.0   110.0    798.0   \n",
       "moments       123.0     41.0     68.0     70.0    10.0    16.0    112.0   \n",
       "...             ...      ...      ...      ...     ...     ...      ...   \n",
       "smg360v         0.0      0.0      0.0      0.0     0.0     0.0      0.0   \n",
       "bobtoeback      0.0      0.0      0.0      0.0     0.0     0.0      0.0   \n",
       "hughugs         0.0      0.0      0.0      0.0     0.0     0.0      0.0   \n",
       "gts5300         0.0      0.0      0.0      0.0     0.0     0.0      0.0   \n",
       "jetplane48      0.0      0.0      0.0      0.0     0.0     0.0      0.0   \n",
       "\n",
       "               ENFP    ISFP    ISTP    ISFJ    ISTJ    ESTP   ESFP   ESTJ  \\\n",
       "Word                                                                        \n",
       "urlweb       1180.0   911.0  1027.0   410.0   402.0   225.0  104.0   36.0   \n",
       "enfp         2134.0   147.0   104.0    53.0   104.0    55.0   39.0   42.0   \n",
       "and         15002.0  5153.0  6540.0  3571.0  3827.0  1905.0  834.0  943.0   \n",
       "intj          576.0   105.0   150.0    77.0    97.0    65.0   41.0   23.0   \n",
       "moments        39.0    11.0    21.0     9.0     7.0     6.0    3.0    0.0   \n",
       "...             ...     ...     ...     ...     ...     ...    ...    ...   \n",
       "smg360v         0.0    26.0     0.0     0.0     0.0     0.0    0.0    0.0   \n",
       "bobtoeback      0.0    10.0     0.0     6.0     0.0     0.0    0.0    0.0   \n",
       "hughugs         0.0    11.0     0.0     0.0     0.0     0.0    0.0    0.0   \n",
       "gts5300         0.0     0.0    11.0     0.0     0.0     0.0    0.0    0.0   \n",
       "jetplane48      0.0     0.0     0.0     0.0     0.0    12.0    0.0    0.0   \n",
       "\n",
       "             ESFJ     all         I    I_perc  all_perc  \n",
       "Word                                                     \n",
       "urlweb       47.0   20887   17279.0  0.002817  0.002615  \n",
       "enfp         35.0    5814    2786.0  0.000454  0.000728  \n",
       "and         988.0  180691  137876.0  0.022481  0.022619  \n",
       "intj         34.0    7806    6217.0  0.001014  0.000977  \n",
       "moments       4.0     540     421.0  0.000069  0.000068  \n",
       "...           ...     ...       ...       ...       ...  \n",
       "smg360v       0.0      26      26.0  0.000004  0.000003  \n",
       "bobtoeback    0.0      16      16.0  0.000003  0.000002  \n",
       "hughugs       0.0      11      11.0  0.000002  0.000001  \n",
       "gts5300       0.0      11      11.0  0.000002  0.000001  \n",
       "jetplane48    0.0      12       0.0  0.000000  0.000002  \n",
       "\n",
       "[17165 rows x 20 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in ['I', 'all']:\n",
    "    df_bow[col+'_perc'] = df_bow[col] / df_bow[col].sum()\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print off the dataframe to view what we've done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>INTP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>all</th>\n",
       "      <th>I</th>\n",
       "      <th>I_perc</th>\n",
       "      <th>all_perc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>67821.0</td>\n",
       "      <td>27381.0</td>\n",
       "      <td>52046.0</td>\n",
       "      <td>43810.0</td>\n",
       "      <td>8875.0</td>\n",
       "      <td>8683.0</td>\n",
       "      <td>87642.0</td>\n",
       "      <td>31156.0</td>\n",
       "      <td>11148.0</td>\n",
       "      <td>13883.0</td>\n",
       "      <td>8037.0</td>\n",
       "      <td>8169.0</td>\n",
       "      <td>3704.0</td>\n",
       "      <td>1696.0</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>2166.0</td>\n",
       "      <td>378073</td>\n",
       "      <td>292556.0</td>\n",
       "      <td>0.047701</td>\n",
       "      <td>0.047328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>39658.0</td>\n",
       "      <td>18993.0</td>\n",
       "      <td>35864.0</td>\n",
       "      <td>30497.0</td>\n",
       "      <td>6132.0</td>\n",
       "      <td>5018.0</td>\n",
       "      <td>48004.0</td>\n",
       "      <td>16454.0</td>\n",
       "      <td>6131.0</td>\n",
       "      <td>8893.0</td>\n",
       "      <td>4111.0</td>\n",
       "      <td>5141.0</td>\n",
       "      <td>2191.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>230224</td>\n",
       "      <td>178299.0</td>\n",
       "      <td>0.029071</td>\n",
       "      <td>0.028820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>40231.0</td>\n",
       "      <td>17852.0</td>\n",
       "      <td>33005.0</td>\n",
       "      <td>28753.0</td>\n",
       "      <td>5889.0</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>48996.0</td>\n",
       "      <td>16945.0</td>\n",
       "      <td>6264.0</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>4607.0</td>\n",
       "      <td>5106.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>227371</td>\n",
       "      <td>175687.0</td>\n",
       "      <td>0.028646</td>\n",
       "      <td>0.028463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>31931.0</td>\n",
       "      <td>14728.0</td>\n",
       "      <td>26692.0</td>\n",
       "      <td>22778.0</td>\n",
       "      <td>4748.0</td>\n",
       "      <td>3966.0</td>\n",
       "      <td>40375.0</td>\n",
       "      <td>13846.0</td>\n",
       "      <td>4825.0</td>\n",
       "      <td>7124.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>4033.0</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>841.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>182870</td>\n",
       "      <td>141091.0</td>\n",
       "      <td>0.023005</td>\n",
       "      <td>0.022892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>31628.0</td>\n",
       "      <td>14236.0</td>\n",
       "      <td>24880.0</td>\n",
       "      <td>21568.0</td>\n",
       "      <td>4564.0</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>40709.0</td>\n",
       "      <td>15002.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>6540.0</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>3827.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>988.0</td>\n",
       "      <td>180691</td>\n",
       "      <td>137876.0</td>\n",
       "      <td>0.022481</td>\n",
       "      <td>0.022619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         INFJ     ENTP     INTP     INTJ    ENTJ    ENFJ     INFP     ENFP  \\\n",
       "Word                                                                         \n",
       "i     67821.0  27381.0  52046.0  43810.0  8875.0  8683.0  87642.0  31156.0   \n",
       "the   39658.0  18993.0  35864.0  30497.0  6132.0  5018.0  48004.0  16454.0   \n",
       "to    40231.0  17852.0  33005.0  28753.0  5889.0  5471.0  48996.0  16945.0   \n",
       "a     31931.0  14728.0  26692.0  22778.0  4748.0  3966.0  40375.0  13846.0   \n",
       "and   31628.0  14236.0  24880.0  21568.0  4564.0  4343.0  40709.0  15002.0   \n",
       "\n",
       "         ISFP     ISTP    ISFJ    ISTJ    ESTP    ESFP    ESTJ    ESFJ  \\\n",
       "Word                                                                     \n",
       "i     11148.0  13883.0  8037.0  8169.0  3704.0  1696.0  1856.0  2166.0   \n",
       "the    6131.0   8893.0  4111.0  5141.0  2191.0   937.0  1000.0  1200.0   \n",
       "to     6264.0   8725.0  4607.0  5106.0  2254.0   972.0  1078.0  1223.0   \n",
       "a      4825.0   7124.0  3333.0  4033.0  1868.0   796.0   841.0   986.0   \n",
       "and    5153.0   6540.0  3571.0  3827.0  1905.0   834.0   943.0   988.0   \n",
       "\n",
       "         all         I    I_perc  all_perc  \n",
       "Word                                        \n",
       "i     378073  292556.0  0.047701  0.047328  \n",
       "the   230224  178299.0  0.029071  0.028820  \n",
       "to    227371  175687.0  0.028646  0.028463  \n",
       "a     182870  141091.0  0.023005  0.022892  \n",
       "and   180691  137876.0  0.022481  0.022619  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow.sort_values(by='all', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vguk_YpZVABC"
   },
   "outputs": [],
   "source": [
    "# calculate chi2\n",
    "df_bow['chi2_i'] = np.power((df_bow['I_perc'] - df_bow['all_perc']), 2) / df_bow['all_perc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOesPcaXVABD",
    "outputId": "b973bca5-2d74-4cdd-9a71-fc9956c0914f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2_i</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>urlweb</th>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infp</th>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infj</th>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infps</th>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infjs</th>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intp</th>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>0.012057</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intps</th>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.047701</td>\n",
       "      <td>0.047328</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.012361</td>\n",
       "      <td>0.012176</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          I_perc  all_perc    chi2_i\n",
       "Word                                \n",
       "urlweb  0.002817  0.002615  0.000016\n",
       "infp    0.001249  0.001118  0.000015\n",
       "infj    0.001111  0.001019  0.000008\n",
       "infps   0.000466  0.000413  0.000007\n",
       "infjs   0.000393  0.000347  0.000006\n",
       "intp    0.000936  0.000871  0.000005\n",
       "my      0.012057  0.011859  0.000003\n",
       "intps   0.000346  0.000315  0.000003\n",
       "i       0.047701  0.047328  0.000003\n",
       "in      0.012361  0.012176  0.000003"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow[['I_perc', 'all_perc', 'chi2_i']][df_bow['I_perc'] > df_bow['all_perc']].sort_values(by='chi2_i', ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dySRM0KqVABE"
   },
   "source": [
    "And there it is! What can we conclude from this?\n",
    "\n",
    "Looking at the top words with higher chi-square values, we can see that words like \"urlweb,\" \"infp,\" \"infj,\" as well as \"i\" have the top chi-square values compared to others. This indicates that these words are used more frequently by introverts than would be expected based on their overall occurrence in the dataset.\n",
    "\n",
    "The word \"I\" appears 9th in the top 10 highest chi-square values of 0.000003, suggesting that its usage by introverts deviates significantly from what would be expected based on its general frequency.\n",
    "\n",
    "Therefore, based on these findings, we can conclude that introverts tend to use \"I\" more frequently than extroverts, supporting the hypothesis that introverts favour the use of the word \"I.\"\n",
    "\n",
    "Let's now have a look at the words most used by extroverts following the same process but for extovert types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2_e</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>enfp</th>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entp</th>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entps</th>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfps</th>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entj</th>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfj</th>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estp</th>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entjs</th>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enfjs</th>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ne</th>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.016918</td>\n",
       "      <td>0.015607</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7w6</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7w8</th>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.003015</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         E_perc  all_perc    chi2_e\n",
       "Word                               \n",
       "enfp   0.001632  0.000728  0.001124\n",
       "entp   0.001474  0.000631  0.001124\n",
       "entps  0.000599  0.000226  0.000619\n",
       "enfps  0.000555  0.000228  0.000468\n",
       "entj   0.000738  0.000360  0.000397\n",
       "enfj   0.000631  0.000356  0.000213\n",
       "estp   0.000517  0.000289  0.000181\n",
       "entjs  0.000241  0.000105  0.000176\n",
       "d      0.000657  0.000424  0.000127\n",
       "enfjs  0.000220  0.000106  0.000122\n",
       "ne     0.000504  0.000312  0.000118\n",
       "you    0.016918  0.015607  0.000110\n",
       "7w6    0.000102  0.000038  0.000107\n",
       "7w8    0.000078  0.000027  0.000098\n",
       "he     0.003015  0.002530  0.000093"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract extrovert types only from all the personality types\n",
    "intro_types_e = [p for p in type_labels if p[0] == 'E']\n",
    "#Create an extrovert total word count column, which sums the counts of all extrovert columns\n",
    "df_bow['E'] = df_bow[intro_types_e].sum(axis=1)\n",
    "#calculate and add percentage column for extroverts\n",
    "df_bow['E_perc'] = df_bow['E'] / df_bow['E'].sum()\n",
    "# calculate chi2 for extroverts\n",
    "df_bow['chi2_e'] = np.power((df_bow['E_perc'] - df_bow['all_perc']), 2) / df_bow['all_perc']\n",
    "df_bow[['E_perc', 'all_perc', 'chi2_e']][df_bow['E_perc'] > df_bow['all_perc']].sort_values(by='chi2_e', ascending=False).head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the chi-squared analysis, there is evidence to suggest that extroverts tend to use words like \"enfp\", \"entp\", \"entps\", and \"enfps\" as well as \"you\" more frequently compared to their overall usage. This supports our hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TlO1q-zlVABg"
   },
   "source": [
    "### n-grams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXZa-xFeVABh"
   },
   "source": [
    "While individual words do carry meaning, it is often the case that combinations of words change meanings of sentences entirely.  For example, what difference does removing the `not` from a sentence make?\n",
    "\n",
    "Natural Language Processing is **not** easy!\n",
    "\n",
    "n-grams are a method to extract combinations of words into features for model building.  The `n` in n-grams specifies the number of tokens to include.  For example, a 2-gram returns all the consecutive pairs of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGJEya0iVABi"
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-W1NOt4VABi"
   },
   "outputs": [],
   "source": [
    "def word_grams(words, min_n=1, max_n=4):\n",
    "    s = []\n",
    "    for n in range(min_n, max_n):\n",
    "        for ngram in ngrams(words, n):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0uyHBnmoVABj",
    "outputId": "c5408f67-e2e2-40a6-a7c4-e31c5c359853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', 'one two', 'two three', 'three four', 'one two three', 'two three four']\n"
     ]
    }
   ],
   "source": [
    "print (word_grams('one two three four'.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine consecutive words into groups of 2 using n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jR-KSHUVABm",
    "outputId": "b2a4e948-4a7b-4c86-9a98-b2f530bd3174"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'find'),\n",
       " ('find', 'all'),\n",
       " ('all', 'of'),\n",
       " ('of', 'you'),\n",
       " ('you', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'extremely'),\n",
       " ('extremely', 'humorous'),\n",
       " ('humorous', 'now'),\n",
       " ('now', 'to'),\n",
       " ('to', 'find'),\n",
       " ('find', 'other'),\n",
       " ('other', 'specimen'),\n",
       " ('specimen', 'to'),\n",
       " ('to', 'observe')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in ngrams(all_mbti.iloc[55555]['tokens'], 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine consecutive words into groups of 3 using n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVUBu-36VABo",
    "outputId": "882045f8-7e10-4f47-d939-bf127459d543"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'find', 'all'),\n",
       " ('find', 'all', 'of'),\n",
       " ('all', 'of', 'you'),\n",
       " ('of', 'you', 'to'),\n",
       " ('you', 'to', 'be'),\n",
       " ('to', 'be', 'extremely'),\n",
       " ('be', 'extremely', 'humorous'),\n",
       " ('extremely', 'humorous', 'now'),\n",
       " ('humorous', 'now', 'to'),\n",
       " ('now', 'to', 'find'),\n",
       " ('to', 'find', 'other'),\n",
       " ('find', 'other', 'specimen'),\n",
       " ('other', 'specimen', 'to'),\n",
       " ('specimen', 'to', 'observe')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in ngrams(all_mbti.iloc[55555]['tokens'], 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2sm4dVWzVABG"
   },
   "source": [
    "## Now that we understand all of that, let's cheat!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUvfk_qrVABG"
   },
   "source": [
    "**Praise be to Python...**\n",
    "\n",
    "`sklearn` has a built in text feature extraction module called [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) that will literally do all of that work in one line of code! This function will convert a collection of documents (rows of text) into a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTbEo8uEVABH"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fe57kvsHVABI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "# Fit the CountVectorizer on the preprocessed 'post' column\n",
    "vect.fit(all_mbti['post'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFzCFS89VABM"
   },
   "source": [
    "### Tuning the vectorizer\n",
    "\n",
    "We have been using the default parameters of [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune with examples on how to do so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqeL2KJ4VABR"
   },
   "source": [
    "- **stop_words:** string 'english', list, or None (default)\n",
    "    * If 'english', a built-in stop word list for English is used.\n",
    "    * If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    * If None, no stop words will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzEOoIt0VABR"
   },
   "outputs": [],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WglDIhfhVABU"
   },
   "source": [
    "- **ngram_range:** tuple (min_n, max_n), default=(1, 1)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "    - All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wx1TyjjVABU"
   },
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfz8TkhbVABW"
   },
   "source": [
    "- **max_df:** float in range [0.0, 1.0] or int, default=1.0\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5B_nbBDVABW"
   },
   "outputs": [],
   "source": [
    "# ignore terms that appear in more than 50% of the documents\n",
    "vect = CountVectorizer(max_df=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7CMv962vVABY"
   },
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.)\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8OcjOK9VABZ"
   },
   "outputs": [],
   "source": [
    "# only keep terms that appear in at least 2 documents\n",
    "vect = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GTogGUiVABa"
   },
   "source": [
    "### Guidelines for tuning CountVectorizer:\n",
    "\n",
    "- Use your knowledge of the **problem** and the **text**, and your understanding of the **tuning parameters**, to help you decide what parameters to tune and how to tune them.\n",
    "- **Experiment**, and let the data tell you the best approach!\n",
    "\n",
    "Finally, let's fit a tuned CountVectorizer to the MBTI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTPnl452VABa"
   },
   "outputs": [],
   "source": [
    "betterVect = CountVectorizer(stop_words='english', \n",
    "                             min_df=2, \n",
    "                             max_df=0.5, \n",
    "                             ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsB226qGVABc",
    "outputId": "d68d31e5-b3d4-4a3f-bb91-97724bf9521e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(max_df=0.5, min_df=2, stop_words=&#x27;english&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_df=0.5, min_df=2, stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(max_df=0.5, min_df=2, stop_words='english')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betterVect.fit(all_mbti['post'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After vectorization using `CountVectorizer`, we can view the transformed data as a matrix where each row represents a document (post in our case) and each column represents a unique word in the vocabulary. The cell values indicate the count of the corresponding word in each document.\n",
    "\n",
    "It's essential to note that this process generates a very large dataset, potentially consuming significant memory on your machine.\n",
    "\n",
    "Uncomment the code below if you would still want to view the vectorized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      00  000  0000  00001010  001  00100000  00100001  00101100  00101110  \\\n",
      "0      0    0     0         0    0         0         0         0         0   \n",
      "1      0    0     0         0    0         0         0         0         0   \n",
      "2      0    0     0         0    0         0         0         0         0   \n",
      "3      0    0     0         0    0         0         0         0         0   \n",
      "4      0    0     0         0    0         0         0         0         0   \n",
      "...   ..  ...   ...       ...  ...       ...       ...       ...       ...   \n",
      "9995   0    0     0         0    0         0         0         0         0   \n",
      "9996   0    0     0         0    0         0         0         0         0   \n",
      "9997   0    0     0         0    0         0         0         0         0   \n",
      "9998   0    0     0         0    0         0         0         0         0   \n",
      "9999   0    0     0         0    0         0         0         0         0   \n",
      "\n",
      "      002  ...  čeština  συντηετιψ  помоћ  послато  са  соционика  тапатока  \\\n",
      "0       0  ...        0          0      0        0   0          0         0   \n",
      "1       0  ...        0          0      0        0   0          0         0   \n",
      "2       0  ...        0          0      0        0   0          0         0   \n",
      "3       0  ...        0          0      0        0   0          0         0   \n",
      "4       0  ...        0          0      0        0   0          0         0   \n",
      "...   ...  ...      ...        ...    ...      ...  ..        ...       ...   \n",
      "9995    0  ...        0          0      0        0   0          0         0   \n",
      "9996    0  ...        0          0      0        0   0          0         0   \n",
      "9997    0  ...        0          0      0        0   0          0         0   \n",
      "9998    0  ...        0          0      0        0   0          0         0   \n",
      "9999    0  ...        0          0      0        0   0          0         0   \n",
      "\n",
      "      уз  ಠಠ  ﾟﾟ  \n",
      "0      0   0   0  \n",
      "1      0   0   0  \n",
      "2      0   0   0  \n",
      "3      0   0   0  \n",
      "4      0   0   0  \n",
      "...   ..  ..  ..  \n",
      "9995   0   0   0  \n",
      "9996   0   0   0  \n",
      "9997   0   0   0  \n",
      "9998   0   0   0  \n",
      "9999   0   0   0  \n",
      "\n",
      "[10000 rows x 50426 columns]\n"
     ]
    }
   ],
   "source": [
    "# Transform the training data\n",
    "vectorized_data = betterVect.transform(all_mbti['post'][0:10000,])\n",
    "\n",
    "# Convert the sparse matrix to a dense array for easier viewing (optional)\n",
    "dense_vectorized_data = vectorized_data.toarray()\n",
    "\n",
    "# Create a DataFrame to display the vectorized data\n",
    "vectorized_df = pd.DataFrame(dense_vectorized_data, columns=betterVect.get_feature_names_out())\n",
    "\n",
    "# Display the vectorized DataFrame\n",
    "print(vectorized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train we covered various techniques for cleaning text data and extracting features to use with machine learning models. We also demonstrated how NLTK's `CountVectorizer` can be used to clean text data and extract features, transforming the text data into a matrix of numbers that can be fed into a machine learning model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VCXae5QXU__Z",
    "wzM8TbWBU__h",
    "FvA-QZmRU__r",
    "hAUkklVXU__6",
    "rFln-NFtVAAI",
    "UZomXVzoVAAR",
    "qp-n688CVAAc",
    "tGmGzrbsVAAf",
    "oFzCFS89VABM",
    "TlO1q-zlVABg"
   ],
   "name": "3_How-do-machines-understand language.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
