{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Exercise.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Natural language processing\n",
    "Â© ExploreAI Academy\n",
    "\n",
    "In this exercise, we will perform text preprocessing tasks such as converting to lowercase, removing punctuation, creating a bag-of-words, and applying stemming and lemmatisation techniques in order to analyse text data to gain some insights.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of this exercise, you should be able to:\n",
    "* Implement text preprocessing techniques such as converting to lowercase and removing punctuation.\n",
    "* Apply stemming and lemmatisation techniques to extract the root forms of words.\n",
    "* Create a bag-of-words representation to quantify the occurrence of words in text.\n",
    "* Calculate statistics such as the number of stopwords, unique words, and word frequencies in text data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import TreebankWordTokenizer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import urllib\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used in this notebook is text from the book \"Alice's Adventures in Wonderland\" by Lewis Carroll. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice's Adventures in Wonderland\n",
      "\n",
      "                ALICE'S ADVENTURES IN WONDERLAND\n",
      "\n",
      "                          Lewis Carroll\n",
      "\n",
      "               THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            CHAPTER I\n",
      "\n",
      "                      Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "  Alice was beginning to get very tired of sitting by her sister\n",
      "on the bank, and of having nothing to do:  once or twice she had\n",
      "peeped into the book her sister was reading, but it had no\n",
      "pictures or conversations in it, `and what is the use of a book,'\n",
      "thought Alice `without pictures or conversation?'\n",
      "\n",
      "  So she was considering in her own mind (as well as she could,\n",
      "for the hot day made her feel very sleepy and stupid), whether\n",
      "the pleasure of making a daisy-chain would be worth the trouble\n",
      "of getting up and picking the daisies, when suddenly a White\n",
      "Rabbit with pink eyes ran close by her.\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# read in the data\n",
    "def print_some_url():\n",
    "    with urllib.request.urlopen('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint//alice_in_wonderland.txt') as f:\n",
    "        return f.read().decode('ISO-8859-1')\n",
    "\n",
    "data = print_some_url()\n",
    "print(data[:863])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "We will first start by providing you with the functions required to remove puntuation, create a bag-of-words and define a stemmer, tokeniser and lemmatiser. Once you apply the functions to pre-process the data, you will be asked to perform some calculations and analysis in the exercise questions below.\n",
    "\n",
    "\n",
    "**Convert to lowercase and remove punctuation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove puntauation\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    words = words.lower()\n",
    "    return ''.join([x for x in words if x not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the remove_punctuation function to the data\n",
    "data = remove_punctuation(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a bag-of-words and assign our stemmer and lemmatiser.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alices': 13,\n",
       " 'adventures': 7,\n",
       " 'wonderland': 4,\n",
       " 'lewis': 1,\n",
       " 'carroll': 1,\n",
       " 'millennium': 1,\n",
       " 'fulcrum': 1,\n",
       " 'edition': 1,\n",
       " '30': 1,\n",
       " 'chapter': 12,\n",
       " 'rabbithole': 3,\n",
       " 'alice': 386,\n",
       " 'beginning': 13,\n",
       " 'get': 46,\n",
       " 'tired': 7,\n",
       " 'sitting': 10,\n",
       " 'sister': 8,\n",
       " 'bank': 2,\n",
       " 'nothing': 34,\n",
       " 'twice': 5,\n",
       " 'peeped': 3,\n",
       " 'book': 7,\n",
       " 'reading': 3,\n",
       " 'pictures': 4,\n",
       " 'conversations': 1,\n",
       " 'use': 18,\n",
       " 'thought': 74,\n",
       " 'without': 25,\n",
       " 'conversation': 10,\n",
       " 'considering': 3,\n",
       " 'mind': 10,\n",
       " 'well': 60,\n",
       " 'could': 77,\n",
       " 'hot': 5,\n",
       " 'day': 25,\n",
       " 'made': 30,\n",
       " 'feel': 8,\n",
       " 'sleepy': 5,\n",
       " 'stupid': 6,\n",
       " 'whether': 11,\n",
       " 'pleasure': 2,\n",
       " 'making': 8,\n",
       " 'daisychain': 1,\n",
       " 'would': 83,\n",
       " 'worth': 4,\n",
       " 'trouble': 6,\n",
       " 'getting': 22,\n",
       " 'picking': 2,\n",
       " 'daisies': 1,\n",
       " 'suddenly': 13,\n",
       " 'white': 30,\n",
       " 'rabbit': 43,\n",
       " 'pink': 1,\n",
       " 'eyes': 27,\n",
       " 'ran': 16,\n",
       " 'close': 13,\n",
       " 'remarkable': 2,\n",
       " 'think': 53,\n",
       " 'much': 51,\n",
       " 'way': 52,\n",
       " 'hear': 14,\n",
       " 'say': 50,\n",
       " 'oh': 38,\n",
       " 'dear': 29,\n",
       " 'shall': 25,\n",
       " 'late': 6,\n",
       " 'afterwards': 2,\n",
       " 'occurred': 2,\n",
       " 'ought': 14,\n",
       " 'wondered': 1,\n",
       " 'time': 68,\n",
       " 'seemed': 27,\n",
       " 'quite': 55,\n",
       " 'natural': 4,\n",
       " 'actually': 1,\n",
       " 'took': 24,\n",
       " 'watch': 8,\n",
       " 'waistcoat': 1,\n",
       " 'pocket': 6,\n",
       " 'looked': 45,\n",
       " 'hurried': 11,\n",
       " 'started': 2,\n",
       " 'feet': 19,\n",
       " 'flashed': 1,\n",
       " 'across': 5,\n",
       " 'never': 45,\n",
       " 'seen': 14,\n",
       " 'either': 10,\n",
       " 'waistcoatpocket': 1,\n",
       " 'take': 21,\n",
       " 'burning': 1,\n",
       " 'curiosity': 5,\n",
       " 'field': 1,\n",
       " 'fortunately': 1,\n",
       " 'see': 66,\n",
       " 'pop': 1,\n",
       " 'large': 33,\n",
       " 'hedge': 2,\n",
       " 'another': 22,\n",
       " 'moment': 29,\n",
       " 'went': 83,\n",
       " 'world': 7,\n",
       " 'straight': 2,\n",
       " 'like': 85,\n",
       " 'tunnel': 1,\n",
       " 'dipped': 2,\n",
       " 'stopping': 1,\n",
       " 'found': 32,\n",
       " 'falling': 2,\n",
       " 'deep': 7,\n",
       " 'fell': 6,\n",
       " 'slowly': 8,\n",
       " 'plenty': 2,\n",
       " 'look': 28,\n",
       " 'wonder': 18,\n",
       " 'going': 27,\n",
       " 'happen': 8,\n",
       " 'next': 30,\n",
       " 'first': 49,\n",
       " 'tried': 19,\n",
       " 'make': 26,\n",
       " 'coming': 9,\n",
       " 'dark': 3,\n",
       " 'anything': 19,\n",
       " 'sides': 4,\n",
       " 'noticed': 8,\n",
       " 'filled': 3,\n",
       " 'cupboards': 2,\n",
       " 'bookshelves': 1,\n",
       " 'saw': 14,\n",
       " 'maps': 1,\n",
       " 'hung': 1,\n",
       " 'upon': 26,\n",
       " 'pegs': 1,\n",
       " 'jar': 2,\n",
       " 'one': 101,\n",
       " 'shelves': 1,\n",
       " 'passed': 5,\n",
       " 'labelled': 1,\n",
       " 'orange': 1,\n",
       " 'marmalade': 1,\n",
       " 'great': 39,\n",
       " 'disappointment': 1,\n",
       " 'empty': 1,\n",
       " 'drop': 1,\n",
       " 'fear': 4,\n",
       " 'killing': 1,\n",
       " 'somebody': 7,\n",
       " 'managed': 4,\n",
       " 'put': 31,\n",
       " 'past': 1,\n",
       " 'fall': 7,\n",
       " 'tumbling': 2,\n",
       " 'stairs': 3,\n",
       " 'brave': 1,\n",
       " 'theyll': 4,\n",
       " 'home': 5,\n",
       " 'wouldnt': 13,\n",
       " 'even': 19,\n",
       " 'top': 8,\n",
       " 'house': 18,\n",
       " 'likely': 5,\n",
       " 'true': 4,\n",
       " 'come': 46,\n",
       " 'end': 18,\n",
       " 'many': 12,\n",
       " 'miles': 3,\n",
       " 'ive': 34,\n",
       " 'fallen': 4,\n",
       " 'said': 462,\n",
       " 'aloud': 5,\n",
       " 'must': 44,\n",
       " 'somewhere': 3,\n",
       " 'near': 15,\n",
       " 'centre': 1,\n",
       " 'earth': 4,\n",
       " 'let': 17,\n",
       " 'four': 6,\n",
       " 'thousand': 2,\n",
       " 'learnt': 2,\n",
       " 'several': 4,\n",
       " 'things': 28,\n",
       " 'sort': 20,\n",
       " 'lessons': 10,\n",
       " 'schoolroom': 1,\n",
       " 'though': 11,\n",
       " 'good': 25,\n",
       " 'opportunity': 8,\n",
       " 'showing': 2,\n",
       " 'knowledge': 3,\n",
       " 'listen': 7,\n",
       " 'still': 13,\n",
       " 'practice': 1,\n",
       " 'yes': 13,\n",
       " 'thats': 33,\n",
       " 'right': 31,\n",
       " 'distancebut': 1,\n",
       " 'latitude': 2,\n",
       " 'longitude': 2,\n",
       " 'got': 45,\n",
       " 'idea': 15,\n",
       " 'nice': 6,\n",
       " 'grand': 3,\n",
       " 'words': 21,\n",
       " 'presently': 2,\n",
       " 'began': 58,\n",
       " 'funny': 3,\n",
       " 'itll': 8,\n",
       " 'seem': 8,\n",
       " 'among': 12,\n",
       " 'people': 13,\n",
       " 'walk': 5,\n",
       " 'heads': 11,\n",
       " 'downward': 1,\n",
       " 'antipathies': 1,\n",
       " 'rather': 25,\n",
       " 'glad': 11,\n",
       " 'listening': 3,\n",
       " 'didnt': 14,\n",
       " 'sound': 4,\n",
       " 'word': 10,\n",
       " 'ask': 11,\n",
       " 'name': 10,\n",
       " 'country': 1,\n",
       " 'know': 86,\n",
       " 'please': 19,\n",
       " 'maam': 1,\n",
       " 'new': 5,\n",
       " 'zealand': 1,\n",
       " 'australia': 1,\n",
       " 'curtsey': 1,\n",
       " 'spokefancy': 1,\n",
       " 'curtseying': 1,\n",
       " 'youre': 23,\n",
       " 'air': 14,\n",
       " 'manage': 7,\n",
       " 'ignorant': 1,\n",
       " 'little': 128,\n",
       " 'girl': 4,\n",
       " 'shell': 3,\n",
       " 'asking': 5,\n",
       " 'perhaps': 17,\n",
       " 'written': 6,\n",
       " 'else': 10,\n",
       " 'soon': 25,\n",
       " 'talking': 17,\n",
       " 'dinahll': 2,\n",
       " 'miss': 4,\n",
       " 'tonight': 1,\n",
       " 'dinah': 11,\n",
       " 'cat': 35,\n",
       " 'hope': 3,\n",
       " 'remember': 14,\n",
       " 'saucer': 1,\n",
       " 'milk': 1,\n",
       " 'teatime': 2,\n",
       " 'wish': 21,\n",
       " 'mice': 3,\n",
       " 'im': 57,\n",
       " 'afraid': 12,\n",
       " 'might': 28,\n",
       " 'catch': 3,\n",
       " 'bat': 3,\n",
       " 'mouse': 38,\n",
       " 'cats': 15,\n",
       " 'eat': 18,\n",
       " 'bats': 4,\n",
       " 'saying': 15,\n",
       " 'dreamy': 1,\n",
       " 'sometimes': 5,\n",
       " 'couldnt': 9,\n",
       " 'answer': 9,\n",
       " 'question': 17,\n",
       " 'matter': 9,\n",
       " 'felt': 23,\n",
       " 'dozing': 1,\n",
       " 'begun': 7,\n",
       " 'dream': 7,\n",
       " 'walking': 5,\n",
       " 'hand': 20,\n",
       " 'earnestly': 2,\n",
       " 'tell': 31,\n",
       " 'truth': 1,\n",
       " 'ever': 21,\n",
       " 'thump': 2,\n",
       " 'came': 40,\n",
       " 'heap': 1,\n",
       " 'sticks': 1,\n",
       " 'dry': 8,\n",
       " 'leaves': 6,\n",
       " 'bit': 16,\n",
       " 'hurt': 3,\n",
       " 'jumped': 6,\n",
       " 'overhead': 1,\n",
       " 'long': 32,\n",
       " 'passage': 4,\n",
       " 'sight': 10,\n",
       " 'hurrying': 1,\n",
       " 'lost': 3,\n",
       " 'away': 25,\n",
       " 'wind': 2,\n",
       " 'turned': 16,\n",
       " 'corner': 3,\n",
       " 'ears': 5,\n",
       " 'whiskers': 3,\n",
       " 'behind': 13,\n",
       " 'longer': 3,\n",
       " 'low': 14,\n",
       " 'hall': 9,\n",
       " 'lit': 1,\n",
       " 'row': 2,\n",
       " 'lamps': 1,\n",
       " 'hanging': 3,\n",
       " 'roof': 6,\n",
       " 'doors': 2,\n",
       " 'round': 41,\n",
       " 'locked': 1,\n",
       " 'side': 17,\n",
       " 'trying': 14,\n",
       " 'every': 12,\n",
       " 'door': 29,\n",
       " 'walked': 10,\n",
       " 'sadly': 5,\n",
       " 'middle': 7,\n",
       " 'wondering': 7,\n",
       " 'threelegged': 2,\n",
       " 'table': 18,\n",
       " 'solid': 1,\n",
       " 'glass': 10,\n",
       " 'except': 4,\n",
       " 'tiny': 4,\n",
       " 'golden': 7,\n",
       " 'key': 9,\n",
       " 'belong': 1,\n",
       " 'alas': 4,\n",
       " 'locks': 2,\n",
       " 'small': 10,\n",
       " 'rate': 9,\n",
       " 'open': 7,\n",
       " 'however': 20,\n",
       " 'second': 4,\n",
       " 'curtain': 1,\n",
       " 'fifteen': 1,\n",
       " 'inches': 6,\n",
       " 'high': 16,\n",
       " 'lock': 1,\n",
       " 'delight': 3,\n",
       " 'fitted': 1,\n",
       " 'opened': 10,\n",
       " 'led': 4,\n",
       " 'larger': 7,\n",
       " 'rathole': 1,\n",
       " 'knelt': 1,\n",
       " 'along': 5,\n",
       " 'loveliest': 1,\n",
       " 'garden': 15,\n",
       " 'longed': 2,\n",
       " 'wander': 1,\n",
       " 'beds': 1,\n",
       " 'bright': 7,\n",
       " 'flowers': 2,\n",
       " 'cool': 2,\n",
       " 'fountains': 2,\n",
       " 'head': 48,\n",
       " 'doorway': 1,\n",
       " 'go': 50,\n",
       " 'poor': 27,\n",
       " 'shoulders': 4,\n",
       " 'shut': 5,\n",
       " 'telescope': 3,\n",
       " 'begin': 13,\n",
       " 'outoftheway': 3,\n",
       " 'happened': 7,\n",
       " 'lately': 1,\n",
       " 'indeed': 16,\n",
       " 'really': 13,\n",
       " 'impossible': 3,\n",
       " 'waiting': 9,\n",
       " 'back': 38,\n",
       " 'half': 21,\n",
       " 'hoping': 3,\n",
       " 'find': 21,\n",
       " 'rules': 3,\n",
       " 'shutting': 2,\n",
       " 'telescopes': 1,\n",
       " 'bottle': 10,\n",
       " 'certainly': 14,\n",
       " 'neck': 7,\n",
       " 'paper': 4,\n",
       " 'label': 2,\n",
       " 'drink': 7,\n",
       " 'beautifully': 2,\n",
       " 'printed': 1,\n",
       " 'letters': 1,\n",
       " 'wise': 2,\n",
       " 'hurry': 11,\n",
       " 'ill': 33,\n",
       " 'marked': 6,\n",
       " 'poison': 3,\n",
       " 'read': 11,\n",
       " 'histories': 1,\n",
       " 'children': 10,\n",
       " 'burnt': 1,\n",
       " 'eaten': 1,\n",
       " 'wild': 2,\n",
       " 'beasts': 2,\n",
       " 'unpleasant': 2,\n",
       " 'simple': 5,\n",
       " 'friends': 2,\n",
       " 'taught': 4,\n",
       " 'redhot': 1,\n",
       " 'poker': 1,\n",
       " 'burn': 2,\n",
       " 'hold': 9,\n",
       " 'cut': 5,\n",
       " 'finger': 5,\n",
       " 'deeply': 4,\n",
       " 'knife': 2,\n",
       " 'usually': 2,\n",
       " 'bleeds': 1,\n",
       " 'forgotten': 6,\n",
       " 'almost': 6,\n",
       " 'certain': 3,\n",
       " 'disagree': 1,\n",
       " 'sooner': 2,\n",
       " 'later': 3,\n",
       " 'ventured': 4,\n",
       " 'taste': 2,\n",
       " 'finding': 3,\n",
       " 'fact': 8,\n",
       " 'mixed': 2,\n",
       " 'flavour': 1,\n",
       " 'cherrytart': 1,\n",
       " 'custard': 1,\n",
       " 'pineapple': 1,\n",
       " 'roast': 1,\n",
       " 'turkey': 1,\n",
       " 'toffee': 1,\n",
       " 'buttered': 1,\n",
       " 'toast': 1,\n",
       " 'finished': 12,\n",
       " 'curious': 19,\n",
       " 'feeling': 7,\n",
       " 'ten': 6,\n",
       " 'face': 14,\n",
       " 'brightened': 2,\n",
       " 'size': 13,\n",
       " 'lovely': 2,\n",
       " 'waited': 11,\n",
       " 'minutes': 11,\n",
       " 'shrink': 1,\n",
       " 'nervous': 5,\n",
       " 'altogether': 5,\n",
       " 'candle': 3,\n",
       " 'fancy': 5,\n",
       " 'flame': 1,\n",
       " 'blown': 1,\n",
       " 'thing': 49,\n",
       " 'decided': 3,\n",
       " 'possibly': 3,\n",
       " 'reach': 4,\n",
       " 'plainly': 1,\n",
       " 'best': 12,\n",
       " 'climb': 1,\n",
       " 'legs': 3,\n",
       " 'slippery': 1,\n",
       " 'sat': 17,\n",
       " 'cried': 20,\n",
       " 'theres': 24,\n",
       " 'crying': 2,\n",
       " 'sharply': 4,\n",
       " 'advise': 1,\n",
       " 'leave': 9,\n",
       " 'minute': 21,\n",
       " 'generally': 7,\n",
       " 'gave': 15,\n",
       " 'advice': 2,\n",
       " 'seldom': 1,\n",
       " 'followed': 8,\n",
       " 'scolded': 1,\n",
       " 'severely': 4,\n",
       " 'bring': 3,\n",
       " 'tears': 11,\n",
       " 'remembered': 5,\n",
       " 'box': 4,\n",
       " 'cheated': 1,\n",
       " 'game': 12,\n",
       " 'croquet': 6,\n",
       " 'playing': 2,\n",
       " 'child': 10,\n",
       " 'fond': 3,\n",
       " 'pretending': 1,\n",
       " 'two': 39,\n",
       " 'pretend': 1,\n",
       " 'hardly': 12,\n",
       " 'enough': 17,\n",
       " 'left': 14,\n",
       " 'respectable': 1,\n",
       " 'person': 4,\n",
       " 'eye': 7,\n",
       " 'lying': 8,\n",
       " 'cake': 3,\n",
       " 'currants': 1,\n",
       " 'makes': 11,\n",
       " 'grow': 13,\n",
       " 'smaller': 3,\n",
       " 'creep': 1,\n",
       " 'dont': 60,\n",
       " 'care': 4,\n",
       " 'happens': 5,\n",
       " 'ate': 1,\n",
       " 'anxiously': 14,\n",
       " 'holding': 3,\n",
       " 'growing': 11,\n",
       " 'surprised': 7,\n",
       " 'remained': 3,\n",
       " 'sure': 24,\n",
       " 'eats': 1,\n",
       " 'expecting': 3,\n",
       " 'dull': 3,\n",
       " 'life': 11,\n",
       " 'common': 1,\n",
       " 'set': 14,\n",
       " 'work': 8,\n",
       " 'ii': 2,\n",
       " 'pool': 10,\n",
       " 'curiouser': 2,\n",
       " 'forgot': 2,\n",
       " 'speak': 14,\n",
       " 'english': 6,\n",
       " 'opening': 3,\n",
       " 'largest': 1,\n",
       " 'goodbye': 1,\n",
       " 'far': 13,\n",
       " 'shoes': 7,\n",
       " 'stockings': 1,\n",
       " 'dears': 3,\n",
       " 'shant': 6,\n",
       " 'able': 1,\n",
       " 'deal': 12,\n",
       " 'kind': 7,\n",
       " 'wont': 24,\n",
       " 'want': 9,\n",
       " 'give': 12,\n",
       " 'pair': 5,\n",
       " 'boots': 4,\n",
       " 'christmas': 1,\n",
       " 'planning': 1,\n",
       " 'carrier': 1,\n",
       " 'sending': 2,\n",
       " 'presents': 2,\n",
       " 'ones': 2,\n",
       " 'odd': 1,\n",
       " 'directions': 3,\n",
       " 'foot': 10,\n",
       " 'esq': 1,\n",
       " 'hearthrug': 1,\n",
       " 'fender': 1,\n",
       " 'love': 3,\n",
       " 'nonsense': 7,\n",
       " 'struck': 2,\n",
       " 'nine': 5,\n",
       " 'hopeless': 1,\n",
       " 'cry': 3,\n",
       " 'ashamed': 2,\n",
       " 'stop': 6,\n",
       " 'shedding': 1,\n",
       " 'gallons': 1,\n",
       " 'reaching': 1,\n",
       " 'heard': 30,\n",
       " 'pattering': 3,\n",
       " 'distance': 7,\n",
       " 'hastily': 16,\n",
       " 'dried': 1,\n",
       " 'returning': 1,\n",
       " 'splendidly': 1,\n",
       " 'dressed': 1,\n",
       " 'kid': 5,\n",
       " 'gloves': 10,\n",
       " 'fan': 10,\n",
       " 'trotting': 2,\n",
       " 'muttering': 3,\n",
       " 'duchess': 39,\n",
       " 'savage': 4,\n",
       " 'kept': 13,\n",
       " 'desperate': 1,\n",
       " 'ready': 8,\n",
       " 'help': 9,\n",
       " 'timid': 3,\n",
       " 'voice': 47,\n",
       " 'sir': 7,\n",
       " 'violently': 4,\n",
       " 'dropped': 5,\n",
       " 'skurried': 1,\n",
       " 'darkness': 1,\n",
       " 'hard': 8,\n",
       " 'fanning': 1,\n",
       " 'queer': 11,\n",
       " 'everything': 10,\n",
       " 'today': 4,\n",
       " 'yesterday': 3,\n",
       " 'usual': 5,\n",
       " 'changed': 8,\n",
       " 'night': 3,\n",
       " 'morning': 5,\n",
       " 'different': 9,\n",
       " 'ah': 5,\n",
       " 'puzzle': 1,\n",
       " 'thinking': 11,\n",
       " 'knew': 14,\n",
       " 'age': 4,\n",
       " 'ada': 1,\n",
       " 'hair': 7,\n",
       " 'goes': 7,\n",
       " 'ringlets': 2,\n",
       " 'mine': 8,\n",
       " 'doesnt': 16,\n",
       " 'cant': 28,\n",
       " 'mabel': 4,\n",
       " 'sorts': 3,\n",
       " 'knows': 2,\n",
       " 'besides': 4,\n",
       " 'shes': 7,\n",
       " 'andoh': 2,\n",
       " 'puzzling': 4,\n",
       " 'try': 12,\n",
       " 'used': 12,\n",
       " 'times': 6,\n",
       " 'five': 8,\n",
       " 'twelve': 4,\n",
       " 'six': 2,\n",
       " 'thirteen': 1,\n",
       " 'seven': 6,\n",
       " 'isoh': 2,\n",
       " 'twenty': 1,\n",
       " 'multiplication': 1,\n",
       " 'signify': 1,\n",
       " 'lets': 5,\n",
       " 'geography': 1,\n",
       " 'london': 1,\n",
       " 'capital': 4,\n",
       " 'paris': 2,\n",
       " 'rome': 1,\n",
       " 'romeno': 1,\n",
       " 'wrong': 5,\n",
       " 'doth': 3,\n",
       " 'crossed': 3,\n",
       " 'hands': 12,\n",
       " 'lap': 2,\n",
       " 'repeat': 7,\n",
       " 'sounded': 5,\n",
       " 'hoarse': 3,\n",
       " 'strange': 5,\n",
       " 'crocodile': 1,\n",
       " 'improve': 1,\n",
       " 'shining': 1,\n",
       " 'tail': 9,\n",
       " 'pour': 1,\n",
       " 'waters': 1,\n",
       " 'nile': 1,\n",
       " 'scale': 1,\n",
       " 'cheerfully': 1,\n",
       " 'seems': 5,\n",
       " 'grin': 6,\n",
       " 'neatly': 2,\n",
       " 'spread': 3,\n",
       " 'claws': 2,\n",
       " 'welcome': 1,\n",
       " 'fishes': 1,\n",
       " 'gently': 3,\n",
       " 'smiling': 2,\n",
       " 'jaws': 2,\n",
       " 'live': 8,\n",
       " 'poky': 1,\n",
       " 'toys': 1,\n",
       " 'play': 8,\n",
       " 'learn': 7,\n",
       " 'stay': 5,\n",
       " 'putting': 3,\n",
       " 'till': 21,\n",
       " 'elsebut': 1,\n",
       " 'sudden': 5,\n",
       " 'burst': 1,\n",
       " 'alone': 4,\n",
       " 'rabbits': 4,\n",
       " 'done': 15,\n",
       " 'measure': 1,\n",
       " 'nearly': 11,\n",
       " 'guess': 3,\n",
       " 'shrinking': 4,\n",
       " 'rapidly': 2,\n",
       " 'cause': 3,\n",
       " 'avoid': 1,\n",
       " 'narrow': 2,\n",
       " 'escape': 4,\n",
       " 'frightened': 7,\n",
       " 'change': 14,\n",
       " 'existence': 1,\n",
       " 'speed': 1,\n",
       " 'worse': 3,\n",
       " 'declare': 2,\n",
       " 'bad': 2,\n",
       " 'slipped': 3,\n",
       " 'splash': 1,\n",
       " 'chin': 7,\n",
       " 'salt': 2,\n",
       " 'water': 4,\n",
       " 'somehow': 1,\n",
       " 'sea': 13,\n",
       " 'case': 5,\n",
       " 'railway': 2,\n",
       " 'seaside': 1,\n",
       " 'general': 3,\n",
       " 'conclusion': 2,\n",
       " 'wherever': 2,\n",
       " 'coast': 1,\n",
       " 'number': 5,\n",
       " 'bathing': 1,\n",
       " 'machines': 1,\n",
       " 'digging': 4,\n",
       " 'sand': 1,\n",
       " 'wooden': 1,\n",
       " 'spades': 1,\n",
       " 'lodging': 1,\n",
       " 'houses': 1,\n",
       " 'station': 1,\n",
       " 'wept': 1,\n",
       " 'hadnt': 8,\n",
       " 'swam': 5,\n",
       " 'punished': 1,\n",
       " 'suppose': 14,\n",
       " 'drowned': 1,\n",
       " 'something': 18,\n",
       " 'splashing': 2,\n",
       " 'nearer': 5,\n",
       " 'walrus': 1,\n",
       " 'hippopotamus': 1,\n",
       " 'talk': 14,\n",
       " 'harm': 1,\n",
       " 'swimming': 2,\n",
       " 'speaking': 5,\n",
       " 'brothers': 1,\n",
       " 'latin': 1,\n",
       " 'grammar': 1,\n",
       " 'mouseof': 1,\n",
       " 'mouseto': 1,\n",
       " 'mousea': 1,\n",
       " 'mouseo': 1,\n",
       " 'inquisitively': 1,\n",
       " 'wink': 2,\n",
       " 'understand': 6,\n",
       " 'daresay': 1,\n",
       " 'french': 4,\n",
       " 'william': 7,\n",
       " 'conqueror': 2,\n",
       " 'history': 7,\n",
       " 'clear': 2,\n",
       " 'notion': 3,\n",
       " 'ago': 2,\n",
       " 'ou': 1,\n",
       " 'est': 1,\n",
       " 'chatte': 1,\n",
       " 'sentence': 6,\n",
       " 'lessonbook': 1,\n",
       " 'leap': 1,\n",
       " 'quiver': 1,\n",
       " 'fright': 2,\n",
       " 'beg': 8,\n",
       " 'pardon': 6,\n",
       " 'animals': 5,\n",
       " 'feelings': 2,\n",
       " 'shrill': 5,\n",
       " 'passionate': 1,\n",
       " 'soothing': 1,\n",
       " 'tone': 40,\n",
       " 'angry': 5,\n",
       " 'yet': 22,\n",
       " 'show': 3,\n",
       " 'youd': 10,\n",
       " 'quiet': 2,\n",
       " 'lazily': 1,\n",
       " 'sits': 1,\n",
       " 'purring': 2,\n",
       " 'nicely': 2,\n",
       " 'fire': 3,\n",
       " 'licking': 1,\n",
       " 'paws': 4,\n",
       " 'washing': 2,\n",
       " 'faceand': 1,\n",
       " 'soft': 1,\n",
       " 'nurseand': 1,\n",
       " 'catching': 2,\n",
       " 'miceoh': 1,\n",
       " 'bristling': 1,\n",
       " 'offended': 10,\n",
       " 'trembling': 6,\n",
       " 'subject': 6,\n",
       " 'family': 1,\n",
       " 'always': 12,\n",
       " 'hated': 1,\n",
       " 'nasty': 1,\n",
       " 'vulgar': 1,\n",
       " 'youare': 1,\n",
       " 'fondofof': 1,\n",
       " 'dogs': 4,\n",
       " 'eagerly': 8,\n",
       " 'dog': 2,\n",
       " 'brighteyed': 1,\n",
       " 'terrier': 1,\n",
       " 'curly': 1,\n",
       " 'brown': 2,\n",
       " 'fetch': 7,\n",
       " 'throw': 3,\n",
       " 'sit': 8,\n",
       " 'dinner': 2,\n",
       " 'thingsi': 1,\n",
       " 'themand': 2,\n",
       " 'belongs': 2,\n",
       " 'farmer': 1,\n",
       " 'says': 4,\n",
       " 'useful': 2,\n",
       " 'hundred': 1,\n",
       " 'pounds': 1,\n",
       " 'kills': 1,\n",
       " 'rats': 1,\n",
       " 'sorrowful': 2,\n",
       " 'commotion': 1,\n",
       " 'called': 15,\n",
       " 'softly': 1,\n",
       " 'pale': 4,\n",
       " 'passion': 3,\n",
       " 'us': 14,\n",
       " 'shore': 3,\n",
       " 'youll': 6,\n",
       " 'hate': 1,\n",
       " 'crowded': 5,\n",
       " 'birds': 9,\n",
       " 'duck': 4,\n",
       " 'dodo': 13,\n",
       " 'lory': 7,\n",
       " 'eaglet': 3,\n",
       " 'creatures': 10,\n",
       " 'whole': 13,\n",
       " 'party': 8,\n",
       " 'iii': 1,\n",
       " 'caucusrace': 3,\n",
       " 'tale': 4,\n",
       " 'queerlooking': 1,\n",
       " 'assembled': 2,\n",
       " 'bankthe': 1,\n",
       " 'draggled': 1,\n",
       " 'feathers': 1,\n",
       " 'fur': 3,\n",
       " 'clinging': 1,\n",
       " 'dripping': 1,\n",
       " 'wet': 2,\n",
       " 'cross': 1,\n",
       " 'uncomfortable': 4,\n",
       " 'course': 25,\n",
       " 'consultation': 1,\n",
       " 'familiarly': 1,\n",
       " 'known': 1,\n",
       " 'argument': 4,\n",
       " 'last': 33,\n",
       " 'sulky': 3,\n",
       " 'older': 2,\n",
       " 'better': 14,\n",
       " 'allow': 3,\n",
       " 'knowing': 2,\n",
       " 'old': 19,\n",
       " 'positively': 1,\n",
       " 'refused': 1,\n",
       " 'authority': 2,\n",
       " 'ring': 2,\n",
       " 'fixed': 1,\n",
       " 'cold': 1,\n",
       " 'ahem': 1,\n",
       " 'important': 5,\n",
       " 'driest': 1,\n",
       " 'silence': 14,\n",
       " 'whose': 2,\n",
       " 'favoured': 1,\n",
       " 'pope': 1,\n",
       " 'submitted': 1,\n",
       " 'wanted': 4,\n",
       " 'leaders': 1,\n",
       " 'accustomed': 1,\n",
       " 'usurpation': 1,\n",
       " 'conquest': 1,\n",
       " 'edwin': 2,\n",
       " 'morcar': 2,\n",
       " 'earls': 2,\n",
       " 'mercia': 2,\n",
       " 'northumbria': 2,\n",
       " 'ugh': 2,\n",
       " 'shiver': 1,\n",
       " 'frowning': 4,\n",
       " 'politely': 6,\n",
       " 'proceed': 2,\n",
       " 'declared': 1,\n",
       " 'stigand': 1,\n",
       " 'patriotic': 1,\n",
       " 'archbishop': 2,\n",
       " 'canterbury': 1,\n",
       " 'advisable': 2,\n",
       " 'replied': 29,\n",
       " 'crossly': 1,\n",
       " 'means': 4,\n",
       " 'frog': 2,\n",
       " 'worm': 1,\n",
       " 'notice': 5,\n",
       " 'hurriedly': 2,\n",
       " 'edgar': 1,\n",
       " 'atheling': 1,\n",
       " 'meet': 2,\n",
       " 'offer': 2,\n",
       " 'crown': 3,\n",
       " 'williams': 1,\n",
       " 'conduct': 1,\n",
       " 'moderate': 1,\n",
       " 'insolence': 1,\n",
       " 'normans': 1,\n",
       " 'continued': 9,\n",
       " 'turning': 12,\n",
       " 'spoke': 16,\n",
       " 'melancholy': 6,\n",
       " 'solemnly': 4,\n",
       " 'rising': 1,\n",
       " 'move': 3,\n",
       " 'meeting': 1,\n",
       " 'adjourn': 1,\n",
       " 'immediate': 1,\n",
       " 'adoption': 1,\n",
       " 'energetic': 1,\n",
       " 'remedies': 1,\n",
       " 'meaning': 8,\n",
       " 'whats': 5,\n",
       " 'believe': 9,\n",
       " 'bent': 1,\n",
       " 'hide': 1,\n",
       " 'smile': 2,\n",
       " 'tittered': 1,\n",
       " 'audibly': 1,\n",
       " 'paused': 1,\n",
       " 'inclined': 1,\n",
       " 'explain': 10,\n",
       " 'winter': 1,\n",
       " 'racecourse': 1,\n",
       " 'circle': 1,\n",
       " 'exact': 1,\n",
       " 'shape': 1,\n",
       " 'placed': 1,\n",
       " 'three': 26,\n",
       " 'running': 8,\n",
       " 'liked': 6,\n",
       " 'easy': 2,\n",
       " 'race': 2,\n",
       " 'hour': 2,\n",
       " 'panting': 2,\n",
       " 'pressed': 3,\n",
       " 'forehead': 2,\n",
       " 'position': 2,\n",
       " 'shakespeare': 1,\n",
       " 'rest': 10,\n",
       " 'everybody': 8,\n",
       " 'prizes': 5,\n",
       " 'chorus': 6,\n",
       " 'voices': 2,\n",
       " 'asked': 17,\n",
       " 'pointing': 4,\n",
       " 'calling': 1,\n",
       " 'confused': 4,\n",
       " 'despair': 1,\n",
       " 'pulled': 1,\n",
       " 'comfits': 2,\n",
       " 'luckily': 2,\n",
       " 'handed': 3,\n",
       " 'exactly': 8,\n",
       " 'apiece': 1,\n",
       " 'prize': 1,\n",
       " 'gravely': 3,\n",
       " 'thimble': 4,\n",
       " 'presented': 1,\n",
       " 'acceptance': 1,\n",
       " 'elegant': 1,\n",
       " 'short': 4,\n",
       " 'speech': 3,\n",
       " 'cheered': 3,\n",
       " 'absurd': 2,\n",
       " 'grave': 3,\n",
       " 'dare': 5,\n",
       " 'laugh': 1,\n",
       " 'simply': 2,\n",
       " 'bowed': 4,\n",
       " 'looking': 31,\n",
       " 'solemn': 3,\n",
       " 'caused': 2,\n",
       " 'noise': 3,\n",
       " 'confusion': 5,\n",
       " 'complained': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define stemmer function\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# tokenise data\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "tokens = tokeniser.tokenize(data)\n",
    "\n",
    "# define lemmatiser\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# bag-of-words\n",
    "def bag_of_words_count(words, word_dict={}):\n",
    "    \"\"\" this function takes in a list of words and returns a dictionary \n",
    "        with each word as a key, and the value represents the number of \n",
    "        times that word appeared\"\"\"\n",
    "    for word in words:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict\n",
    "\n",
    "# remove stopwords\n",
    "tokens_less_stopwords = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "# create bag of words\n",
    "bag_of_words = bag_of_words_count(tokens_less_stopwords)\n",
    "bag_of_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay special attention to what these functions return and how the subsequent texts and lists look."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the stemmer and lemmatiser functions (defined in the cells above) from the relevant library to write a function that finds the stem and lemma of the nth word in the token list.\n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Should take a `list` as input and return a  `dict` type as output.\n",
    "* The dictionary should have the keys **'original',  'stem' and 'lemma'** with the corresponding values being the nth word transformed in that way.\n",
    "\n",
    "**Example result:**\n",
    "\n",
    "`{'original': 'daisies', \n",
    "'stem': 'daisi', \n",
    "'lemma': 'daisy'}`\n",
    "\n",
    "Use your function to find the 120th word in `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "def nth_stem_and_lemma(n, tokens):\n",
    "    '''\n",
    "    Returns the stem and lemma of the nth element in a list of tokens\n",
    "\n",
    "    Args:\n",
    "    - n: Number of element (1-based)\n",
    "    - tokens: List of tokens\n",
    "\n",
    "    Return:\n",
    "    - Dictionary with the keys 'original', 'stem' and 'lemma' with \n",
    "    the corresponding values being the nth word transformed in that way\n",
    "    '''\n",
    "\n",
    "    word = tokens[n-1]\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "\n",
    "    return {'original': word, 'stem': stemmed, 'lemma': lemmatized}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': 'daisies', 'stem': 'daisi', 'lemma': 'daisy'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nth_stem_and_lemma(120, tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that calculates the number of stopwords that are in the text in total, including repetitions.   \n",
    "\n",
    "_Hint_ : you can use the nltk stopwords dictionary \n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Function should take a `list` as input \n",
    "* The number of stopwords should be returned as an `int` \n",
    "\n",
    "Use your function to calculate the total number of stopwords in `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13774"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "def number_of_stopwords(tokens):\n",
    "    '''\n",
    "    Gets the total number of stopwords in a token list (including repetitions)\n",
    "\n",
    "    Args:\n",
    "    - tokens: List of tokens\n",
    "\n",
    "    Returns:\n",
    "    - Total number of stopwords as int\n",
    "    '''\n",
    "\n",
    "    total = 0\n",
    "    for token in tokens:\n",
    "        if token in stopwords.words('english'):\n",
    "            total += 1\n",
    "\n",
    "    return total\n",
    "number_of_stopwords(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Write a function that calculates the number of **unique** words in the text.\n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Function should take a `list` as input and return an `int` \n",
    "\n",
    "\n",
    "Use your function to calculate the number of **unique** words in `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "def unique_words(tokens):\n",
    "    return len(set(tokens))\n",
    "unique_words(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that calculates the kth most frequently occuring word in the bag of words.\n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Function should take a `dict` and an `int` k as input\n",
    "* Function should return the kth most common word as a `str`\n",
    "\n",
    "_Hint : bag_of_words already does not include stopwords_\n",
    "\n",
    "**Example input:**\n",
    "```python\n",
    "most_common_word(bag = {'apple': 30, 'orange': 12, 'pear': 50, 'banana': 12}, 2)\n",
    "\n",
    ">>> 'apple'\n",
    "```\n",
    "\n",
    "\n",
    "Use the function to calculate the 3rd most frequently occuring word in the bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'little'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "import pandas as pd\n",
    "\n",
    "def most_common_word(bag, k):\n",
    "    df = pd.DataFrame(list(bag.items()), columns=['word', 'count'])\n",
    "    return df.sort_values(by='count', ascending=False).iloc[k-1, 0]\n",
    "\n",
    "most_common_word(bag_of_words, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Write a function that calculates the number of words that appear n times in the text.\n",
    "\n",
    "_**Function Specifications:**_\n",
    "* Input is taken as a `dict` and an `int` n, where n is the number of times the word appears in the text\n",
    "* Count the number of words that appear n times in the text\n",
    "* Output should be the count as an `int`\n",
    "\n",
    "**Example input:** \n",
    "```python\n",
    "word_frequency_count(bag = {'apple': 30, 'orange': 12, 'pear': 50, 'banana': 12}, 12)\n",
    "\n",
    ">>> 2\n",
    "```\n",
    "\n",
    "Use the function to calculate the number of words that appear 8 times in the bag-of-words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "def word_frequency_count(bag, n):\n",
    "    df = pd.DataFrame(list(bag.items()), columns=['word', 'count'])\n",
    "    return df[df['count'] == n].shape[0]\n",
    "\n",
    "word_frequency_count(bag_of_words, 8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_roots(token_list, n):\n",
    "    \n",
    "    root_dict = {}\n",
    "    word = token_list[n-1]\n",
    "    root_dict['original'] = word\n",
    "    root_dict[\"stem\"] = stemmer.stem(word)\n",
    "    root_dict[\"lemma\"] = lemmatizer.lemmatize(word)\n",
    "    \n",
    "    return root_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_roots(tokens, 120) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stopwords(token_list):\n",
    "    STOPwords = [word for word in token_list if word in stopwords.words(\"english\")]\n",
    "    return len(STOPwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13774"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_stopwords(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(token_list):\n",
    "    return len(set(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The same result can be achieved by using the `len()` function on the `bag_of_words_count()` function to calculate the number of unique words in `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bag_of_words_count(tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_word(bag, k):\n",
    "    switch = [(value, key) for key, value in bag.items()]\n",
    "    switch = sorted(switch)\n",
    "    return switch[-k][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'little'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_word(bag_of_words, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency_count(bag, n):\n",
    "    total = sum(1 for value in bag.values() if value == n)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency_count(bag_of_words, 8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
